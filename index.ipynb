{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from taitac.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# taitac\n",
    "\n",
    "> Trusted AI -- Towards an AI Curator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Trusted AI - Towards a Curator (TAITaC) is an initiative dedicated to constructing metadata and curating data crucial for the development of AI and for scientific research at large. This project is birthed from the recognition that the landscape of AI and scientific research thrives on reliable data, and that constructing reliable metadata and curating such data automatically can revolutionize the way we approach research and AI model development.\n",
    "\n",
    "TAITaC is part of the Notre Dame Trusted AI Knowledge Engineering project, TAITAC aims to explore the architectures needed for \"Curator AI's\". The project's home is the `nd-crane/taitac` repository on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features for AI Agents\n",
    "\n",
    "1. **Automatic Metadata Generation**: Construct rich metadata for AI development, capturing necessary details and relationships.\n",
    "2. **Data Curation**: Automate the process of sorting, categorizing, and maintaining data, ensuring AI models and researchers have access to the best quality data.\n",
    "3. **Curator AI Architectures Experimentation**: Experiment and iterate on potential AI architectures to best serve the goals of data curation and metadata generation.\n",
    "\n",
    "## The Need for AI Curators\n",
    "\n",
    "![eu-interoperability-AI.png](./images/eu-interoperability-AI.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technology Stack\n",
    "\n",
    "- **Language**: Python\n",
    "- **Development Framework**: [nbdev](https://nbdev.fast.ai/)\n",
    "- **Environment Management**: [miniforge](https://github.com/conda-forge/miniforge)\n",
    "- **Development Environment**: [devcontainers](https://code.visualstudio.com/docs/remote/containers)\n",
    "- **AI Interaction**: [llm python library](https://github.com/simonw/llms) (For Large Language Model APIs interaction)\n",
    "- **Knowledge Graph**: [rdflib](https://rdflib.readthedocs.io/en/stable/index.html) (For RDF graph construction and querying)\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Install [Python 3.9](https://www.python.org/downloads/) or higher.\n",
    "- Install [miniforge](https://github.com/conda-forge/miniforge#download).\n",
    "- Recommended: [Visual Studio Code](https://code.visualstudio.com/) with [devcontainers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers).\n",
    "\n",
    "### Setup and Installation\n",
    "\n",
    "1. Clone the `nd-crane/taitac` repository:\n",
    "   ```\n",
    "   git clone https://github.com/nd-crane/taitac.git\n",
    "   cd taitac\n",
    "   ```\n",
    "\n",
    "2. Set up the devcontainer environment (if using VS Code):\n",
    "   - Open VS Code and open the project folder.\n",
    "   - When prompted, reopen the folder in the devcontainer.\n",
    "\n",
    "3. Create a conda environment using miniforge:\n",
    "   ```\n",
    "   conda create -n taitac_env python=3.9\n",
    "   conda activate taitac_env\n",
    "   ```\n",
    "\n",
    "4. Install the required dependencies:\n",
    "   ```\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "### Usage\n",
    "\n",
    "_To be added as the project progresses._\n",
    "\n",
    "## Contribution\n",
    "\n",
    "We welcome contributions to the TAITAC project! If you'd like to contribute, please see our [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines and details.\n",
    "\n",
    "## License\n",
    "\n",
    "This project is licensed under the [MIT License](./LICENSE).\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- Notre Dame Trusted AI Knowledge Engineering project for providing the foundational context.\n",
    "- All contributors and supporters of the project.\n",
    "\n",
    "---\n",
    "\n",
    "For more details, updates, and discussions, please refer to the official [GitHub repository](https://github.com/nd-crane/taitac) and the associated issues and pull requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "Our approach is a comprehensive blend of modern technologies and strategies to build a robust, interactive, and intelligent data platform. At its core, it's an ecosystem that harmonizes Knowledge Graphs (KG) with cognitive capabilities of Large Language Models (LLM), streamlined by efficient data interfaces like SPARQL, JSON-LD, and OpenAPI.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "### 1. Knowledge Graph (KG) Storage\n",
    "\n",
    "#### Storage Layer\n",
    "- Purpose: Store RDF triples/quads, which form the backbone of our knowledge representation.\n",
    "- Technology: A specialized database optimized for RDF storage ensuring speed, scalability, and efficient querying.\n",
    "\n",
    "#### SPARQL Endpoint\n",
    "- Purpose: Provide an interface for querying the KG.\n",
    "- Technology: A standard SPARQL endpoint ensuring compatibility with various tools and services.\n",
    "\n",
    "### 2. Microservices Layer\n",
    "\n",
    "#### SPARQL Service\n",
    "- Purpose: Interface with the KG to fetch relevant data.\n",
    "- Features:\n",
    "  - Fetch data using SPARQL queries.\n",
    "  - Convert query results, primarily to JSON-LD format for interoperability.\n",
    "\n",
    "#### OpenAPI Service\n",
    "- Purpose: Expose data and functionalities as RESTful services.\n",
    "- Features:\n",
    "  - Standard REST API endpoint allowing CRUD operations.\n",
    "  - Integration points for third-party services and applications.\n",
    "\n",
    "#### LLM Cognitive Agent Service\n",
    "- Purpose: Provide intelligent, natural language processing capabilities.\n",
    "- Features:\n",
    "  - Process and understand natural language queries.\n",
    "  - Interact with SPARQL and OpenAPI services.\n",
    "  - Return comprehensive, human-friendly responses.\n",
    "  - Learn and refine its knowledge based on continuous feedback.\n",
    "\n",
    "### 3. Integration Middleware\n",
    "\n",
    "#### JSON-LD Processor\n",
    "- Purpose: Ensure semantic interoperability.\n",
    "- Features:\n",
    "  - Convert data between JSON-LD and other formats.\n",
    "  - Support semantic annotations and context-aware data transformations.\n",
    "\n",
    "#### Translator\n",
    "- Purpose: Convert user queries into actionable data requests.\n",
    "- Features:\n",
    "  - Translate natural language queries into SPARQL or OpenAPI requests.\n",
    "  - Employ contextual understanding to optimize data retrieval strategies.\n",
    "\n",
    "### 4. User Interface (UI)\n",
    "\n",
    "#### Web UI\n",
    "- Purpose: Offer users an interactive platform to engage with the system.\n",
    "- Features:\n",
    "  - Send natural language queries.\n",
    "  - Visualize KG data.\n",
    "  - Receive and review responses from the LLM.\n",
    "\n",
    "#### REST API Client\n",
    "- Purpose: Programmable interface for developer interactions.\n",
    "- Features:\n",
    "  - Access to all underlying services for third-party integrations.\n",
    "  - Build atop the platform for custom applications and utilities.\n",
    "\n",
    "### 5. Feedback Mechanism\n",
    "\n",
    "- Purpose: Continuously refine the LLM's knowledge and accuracy.\n",
    "- Features:\n",
    "  - Users can provide feedback on LLM responses.\n",
    "  - The feedback loop informs the learning process of the LLM Cognitive Agent Service.\n",
    "\n",
    "### 6. Security and Authentication\n",
    "\n",
    "- Purpose: Ensure safe and authorized access to data and services.\n",
    "- Features:\n",
    "  - Secure the OpenAPI and SPARQL endpoints.\n",
    "  - Implement strategies like rate limiting, API keys, or OAuth mechanisms.\n",
    "\n",
    "### 7. Scalability and Load Balancing\n",
    "\n",
    "- Purpose: Handle large volumes of data and high concurrency gracefully.\n",
    "- Features:\n",
    "  - Distribute incoming traffic with load balancers.\n",
    "  - Scale storage solutions based on data growth and query demands.\n",
    "\n",
    "### 8. Logging and Monitoring\n",
    "\n",
    "- Purpose: Maintain system health, track interactions, and optimize performance.\n",
    "- Features:\n",
    "  - Monitor service health and performance metrics.\n",
    "  - Log interactions, errors, and anomalies for audit and continuous improvement.\n",
    "\n",
    "---\n",
    "\n",
    "The proposed architecture ensures that users can fluidly interact with a vast repository of knowledge using natural language. By converting complex queries into actionable data points and leveraging the cognitive prowess of LLMs, the system is designed to deliver accurate, concise, and human-readable responses. In essence, it's a forward-looking blueprint that envisions a seamless blend of structured KGs with the intuitive cognition of LLMs, underpinned by robust, scalable, and secure services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Architecture\n",
    "\n",
    "### Initial Experiment Architecture from AVIS.\n",
    "\n",
    "[AVIS: Autonomous Visual Information Seeking with Large Language Model Agent](https://arxiv.org/abs/2306.08129) is a novel framework for visual question answering that requires external knowledge. It consists of three components: a planner, a reasoner, and a working memory. The planner decides which tool to use next, such as web search, image search, or computer vision. The reasoner analyzes the output of the tool and extracts relevant information. The working memory stores the acquired information and updates it as the process progresses. AVIS uses a large language model to power both the planner and the reasoner, and leverages user behavior data to guide its decision-making.\n",
    "\n",
    "The tool use component of AVIS is responsible for selecting the most appropriate external tool to obtain information from, based on the current state and the question. AVIS supports three types of tools: web search, image search, and computer vision. Web search queries a search engine with keywords extracted from the question and returns a list of web pages. Image search queries an image search engine with the image provided by the user and returns a list of similar images. Computer vision applies a pre-trained object detection model to the image and returns a list of detected objects and their locations.\n",
    "\n",
    "The state graph component of AVIS is responsible for defining the possible states and transitions of the information seeking process. A state is a representation of the current knowledge acquired by AVIS, such as the question, the image, the tool outputs, and the working memory. A transition is a change of state triggered by an action, such as invoking a tool or updating the working memory. The state graph is constructed by analyzing the user behavior data collected in a user study, where human participants were asked to answer visual questions that require external knowledge using various tools. The state graph serves as a constraint for the planner to choose valid actions at each state.\n",
    "\n",
    "A summary of the Architecture is available from the Google Research Blog [Autonomous visual information seeking with large language models](https://blog.research.google/2023/08/autonomous-visual-information-seeking.html).\n",
    "\n",
    "\n",
    "![AVIS Architecture](images/AVIS_arch.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Use Case for AI Curator -- CSV Column Type Annotation\n",
    "[Column Type Annotation using ChatGPT](https://arxiv.org/abs/2306.00745) is a novel approach to annotate the semantic types of table columns using a large language model (LLM) called ChatGPT¹[1]. The proposed method is based on the idea that a language model can be used to generate natural language descriptions of table columns, which can then be used to infer their semantic types. The method is evaluated on the WikiTableQuestions dataset²[2] and achieves state-of-the-art performance in zero-shot and few-shot settings.\n",
    "\n",
    "- **Column Type Annotation using ChatGPT**: A novel approach to annotate the semantic types of table columns using a large language model (LLM) called ChatGPT¹[1].\n",
    "- **Prompt Design and Evaluation**: Different ways to formulate prompts for the column type annotation (CTA) task and their performance in zero- and few-shot settings.\n",
    "- **Explicit Instructions and Message Roles**: How to improve the performance of ChatGPT by providing step-by-step instructions and using message roles to distinguish between system, user, and AI messages²[2].\n",
    "- **In-Context Learning**: How to further boost the performance of ChatGPT by providing task demonstrations as part of the prompt in one-shot and five-shot setups.\n",
    "- **Two-Step Pipeline**: A proposed method to deal with large label spaces by first predicting the domain of the table and then using only the relevant subset of labels for CTA³[3].\n",
    "- **Comparison to Baselines**: A comparison of ChatGPT to state-of-the-art CTA methods based on pre-trained language models (PLMs) such as RoBERTa and DODUO.\n",
    "\n",
    "## CSV Datasets for AI Curator\n",
    "[SOTAB V2 - Table Annotation Benchmark](http://webdatacommons.org/structureddata/sotab/v2/) Code for constructing the benchmark: (https://github.com/wbsg-uni-mannheim/wdc-sotab):\n",
    "[GitTables: a large-scale corpus of relational tables download.](https://gittables.github.io/)\n",
    "[GitTables: A Large-Scale Corpus of Relational Tables](https://arxiv.org/pdf/2106.07258.pdf) -- [GitHub](https://github.com/madelonhulsebos/gittables)\n",
    "[GitTables Download](https://zenodo.org/records/4943312)\n",
    "\n",
    "- **SOTAB V2**: A benchmark for **table annotation** using **Schema.org** and **DBpedia** terms. It covers two tasks: **Column Type Annotation (CTA)** and **Columns Property Annotation (CPA)**¹[1]. It consists of **45,834 tables** annotated for CTA and **30,220 tables** annotated for CPA from **55,511 websites**²[2].\n",
    "- **Table annotation tasks**: The goal of CTA is to assign a type to each table column, such as telephone, Duration, or Organization³[3]. The goal of CPA is to assign a property to each pair of columns, such as gtin, startDate, or recipeIngredient⁴[4].\n",
    "- **Table annotation challenges**: The benchmark includes subsets of test tables that measure the performance of table annotation systems on specific challenges, such as missing values, value format heterogeneity, and corner cases.\n",
    "- **Baseline methods**: Three methods are used to evaluate the benchmark: a non-deep learning method based on TF-IDF and Random Forest, a deep learning method called TURL that uses Transformer and TinyBERT, and a deep learning method called DODUO that uses BERT and table serialization.\n",
    "- **Download and code**: The benchmark datasets and the code for building the benchmark are available for public download on github⁵[5]. The datasets are provided in JSON and CSV formats."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
