{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from taitac.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# taitac\n",
    "\n",
    "> Trusted AI -- Towards an AI Curator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Trusted AI - Towards a Curator (TAITaC) is an initiative dedicated to constructing metadata and curating data crucial for the development of AI and for scientific research at large. This project is birthed from the recognition that the landscape of AI and scientific research thrives on reliable data, and that constructing reliable metadata and curating such data automatically can revolutionize the way we approach research and AI model development.\n",
    "\n",
    "TAITaC is part of the Notre Dame Trusted AI Knowledge Engineering project, TAITAC aims to explore the architectures needed for \"Curator AI's\". The project's home is the `nd-crane/taitac` repository on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features for AI Agents\n",
    "\n",
    "1. **Automatic Metadata Generation**: Construct rich metadata for AI development, capturing necessary details and relationships.\n",
    "2. **Data Curation**: Automate the process of sorting, categorizing, and maintaining data, ensuring AI models and researchers have access to the best quality data.\n",
    "3. **Curator AI Architectures Experimentation**: Experiment and iterate on potential AI architectures to best serve the goals of data curation and metadata generation.\n",
    "\n",
    "## The Need for AI Curators\n",
    "\n",
    "[Tangi, Luca, Marco Combetto, BOSCH Jaume Martin, and MÜLLER Paula Rodriguez. 2023. “Artificial Intelligence for Interoperability in the European Public Sector.” JRC Publications Repository. October 4, 2023.](https://doi.org/10.2760/633646).\n",
    "\n",
    "> \"Moreover, the semantic interoperability layer is fundamental in most of the cases. In addition, ontologies and taxonomies combined with AI can help in establishing interoperability between different systems. The solutions analysed classify, detect and provide structure, among other actions performed on data. Hence, AI has the capability to standardise, clean, structure and increase the usage of large volumes of data, thus improving overall quality and making it easier to use and share between different systems.\"\n",
    "\n",
    "![eu-interoperability-AI.png](./images/eu-interoperability-AI.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technology Stack\n",
    "\n",
    "- **Language**: Python\n",
    "- **Development Framework**: [nbdev](https://nbdev.fast.ai/)\n",
    "- **Environment Management**: [miniforge](https://github.com/conda-forge/miniforge)\n",
    "- **Development Environment**: [devcontainers](https://code.visualstudio.com/docs/remote/containers)\n",
    "- **AI Interaction**: [llm python library](https://github.com/simonw/llms) (For Large Language Model APIs interaction)\n",
    "- **Knowledge Graph**: [rdflib](https://rdflib.readthedocs.io/en/stable/index.html) (For RDF graph construction and querying)\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Install [Python 3.9](https://www.python.org/downloads/) or higher.\n",
    "- Install [miniforge](https://github.com/conda-forge/miniforge#download).\n",
    "- Recommended: [Visual Studio Code](https://code.visualstudio.com/) with [devcontainers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers).\n",
    "\n",
    "### Setup and Installation\n",
    "\n",
    "1. Clone the `nd-crane/taitac` repository:\n",
    "   ```\n",
    "   git clone https://github.com/nd-crane/taitac.git\n",
    "   cd taitac\n",
    "   ```\n",
    "\n",
    "2. Set up the devcontainer environment (if using VS Code):\n",
    "   - Open VS Code and open the project folder.\n",
    "   - When prompted, reopen the folder in the devcontainer.\n",
    "\n",
    "3. Create a conda environment using miniforge:\n",
    "   ```\n",
    "   conda create -n taitac_env python=3.9\n",
    "   conda activate taitac_env\n",
    "   ```\n",
    "\n",
    "4. Install the required dependencies:\n",
    "   ```\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "### Usage\n",
    "\n",
    "_To be added as the project progresses._\n",
    "\n",
    "## Contribution\n",
    "\n",
    "We welcome contributions to the TAITAC project! If you'd like to contribute, please see our [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines and details.\n",
    "\n",
    "## License\n",
    "\n",
    "This project is licensed under the [MIT License](./LICENSE).\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- Notre Dame Trusted AI Knowledge Engineering project for providing the foundational context.\n",
    "- All contributors and supporters of the project.\n",
    "\n",
    "---\n",
    "\n",
    "For more details, updates, and discussions, please refer to the official [GitHub repository](https://github.com/nd-crane/taitac) and the associated issues and pull requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "Our approach is a comprehensive blend of modern technologies and strategies to build a robust, interactive, and intelligent data platform. At its core, it's an ecosystem that harmonizes Knowledge Graphs (KG) with cognitive capabilities of Large Language Models (LLM), streamlined by efficient data interfaces like SPARQL, JSON-LD, and OpenAPI.\n",
    "\n",
    "## LLM Architecture Patterns for Agents\n",
    "We roughly follow the unified framework for LLMs proposed by Wang et al. (2021) to build our agents. The framework is shown below:\n",
    "\n",
    "![LLM Architecture Patterns](./images/LLM-Architecture-Patterns.png)\n",
    "\n",
    "[Wang, Lei, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, et al. 2023. “A Survey on Large Language Model Based Autonomous Agents.” arXiv.](http://arxiv.org/abs/2308.11432)\n",
    "\n",
    "## OpenAI Assistants API and Python SDK\n",
    "\n",
    "OpenAI has announced a Agents framework called the [Assistants - OpenAI API](https://platform.openai.com/docs/assistants/overview) that allows users to build agents that can interact with humans. The components of the assistants API roughly decomposes to the following Agent Architectural Patterns:\n",
    "\n",
    "| Unified Framework Component | Assistants API Equivalent | Description                                                                                         |\n",
    "|-----------------------------|----------------------------|-----------------------------------------------------------------------------------------------------|\n",
    "| Profile Module              | Assistant                 | Defines the AI's attributes, capabilities, and personality, setting the stage for its interactions. |\n",
    "| Memory Module               | Thread                    | Manages the history of interactions, storing and truncating messages to maintain context.           |\n",
    "| Planning Module             | Run                       | Processes the current state and plans the next actions, deciding how to utilize messages and tools. |\n",
    "| Action Module               | Message & Run Step        | Executes actions, generates responses, and may call tools to produce outputs or perform tasks.      |\n",
    "\n",
    "State transitions in the Assistants API would involve changes in the \"Thread\" as new \"Messages\" are added, and as the \"Assistant\" performs new \"Runs\" and \"Run Steps,\" updating its current knowledge and planned actions.\n",
    "\n",
    "In addition to these framework components, the Assistants API also provides a \"Tool\" component that allows the agent to perform tasks and produce outputs through [OpenAI Functions](https://platform.openai.com/docs/guides/function-calling) and Retrieval Augmented Generation (RAG) models through document upload.\n",
    "\n",
    "OpenAI has added python SDK examples to it's cookbook [Assistants API Overview (Python SDK)](https://cookbook.openai.com/examples/assistants_api_overview_python) and LLamaIndex has integrated [examples of using the Assistants API](https://github.com/run-llama/llama_index/tree/main/docs/examples/agent) including [ReAct](https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/react_agent.ipynb) Agents.\n",
    "\n",
    "## Planning and Action Modules\n",
    "Simon Willison has a very simple blog post of implementing the [ReAct: Synergizing Reasoning and Acting in Language Models](https://react-lm.github.io/) in one of his TILs -- [A simple Python implementation of the ReAct pattern for LLMs](https://til.simonwillison.net/llms/python-react-pattern). There are some issues in using regular expressions to select the tool the LLM want's to use. If the response can be constrained using a grammar as with the OpenAI model, more expressive response options are available. \n",
    "\n",
    "The issue with the ReAct pattern is that it is course grained planning and action. A new approach, called [ADaPT: As-Needed Decomposition and Planning with Language Models](https://allenai.github.io/adaptllm/) allows recursive decomposition of the planning and action steps as needed by supporting logical operators within the task planning and execution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts for Column Type Extraction\n",
    "[Column Type Annotation using ChatGPT](https://arxiv.org/abs/2306.00745) is a novel approach to annotate the semantic types of table columns using a large language model (LLM) called ChatGPT¹[1]. The proposed method is based on the idea that a language model can be used to generate natural language descriptions of table columns, which can then be used to infer their semantic types. The method is evaluated on the WikiTableQuestions dataset²[2] and achieves state-of-the-art performance in zero-shot and few-shot settings.\n",
    "\n",
    "- **Column Type Annotation using ChatGPT**: A novel approach to annotate the semantic types of table columns using a large language model (LLM) called ChatGPT¹[1].\n",
    "- **Prompt Design and Evaluation**: Different ways to formulate prompts for the column type annotation (CTA) task and their performance in zero- and few-shot settings.\n",
    "- **Explicit Instructions and Message Roles**: How to improve the performance of ChatGPT by providing step-by-step instructions and using message roles to distinguish between system, user, and AI messages²[2].\n",
    "- **In-Context Learning**: How to further boost the performance of ChatGPT by providing task demonstrations as part of the prompt in one-shot and five-shot setups.\n",
    "- **Two-Step Pipeline**: A proposed method to deal with large label spaces by first predicting the domain of the table and then using only the relevant subset of labels for CTA³[3].\n",
    "- **Comparison to Baselines**: A comparison of ChatGPT to state-of-the-art CTA methods based on pre-trained language models (PLMs) such as RoBERTa and DODUO.\n",
    "\n",
    "## DataSets for testing data curation\n",
    "The [GitTables: A Large-Scale Corpus of Relational Tables](https://arxiv.org/pdf/2106.07258.pdf) has constructed a test data set of 1.5 million tables from GitHub. The data set is available from [GitTables](\n",
    "- [GitTables benchmark - column type detection](https://zenodo.org/records/5706316)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Evaluation\n",
    "OpenAI uses the [Evals Framework](https://github.com/openai/evals) for unit and behavior testing LLMs as a potential model for applying trusted AI to the models. A informative blog post is at [Mastering OpenAI’s ‘evals’: A Deep Dive into Evaluating LLMs](https://medium.com/@sergioli/evaluating-chatgpt-using-openai-evals-7ca85c0ad139) and OpenAI touched on them during developer days [The New Stack and Ops for AI](https://youtu.be/XGJNo8TpuVA?si=Yyh6dKTXouEXxvMf). LlamaIndex also has a section in it's documentation on [Evaluation](https://gpt-index.readthedocs.io/en/stable/optimizing/evaluation/evaluation.html). LLamaIndex can use [deepeval](https://github.com/confident-ai/deepeval). RAG pipelines can also be evaluated with [Ragas](https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/).\n",
    " \n",
    "### Types of mistakes to build evals for\n",
    "- Bad outputting format\n",
    "- Inaccurate responses/actions\n",
    "- \"Going off the rails\"\n",
    "- Bad tone\n",
    "- Hallucinations\n",
    "\n",
    "### Evaulating Evals\n",
    " - Model-graded evals -- use GPT4 as a grader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
