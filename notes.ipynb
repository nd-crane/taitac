{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import ChatCompletion,Completion\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from fastcore.utils import *\n",
    "from pathlib import Path\n",
    "from os import getenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not load_dotenv(\".env\"):\n",
    "    raise ValueError(\"EnvironmentError: Failed to load `.env`\")\n",
    "\n",
    "api_key = getenv(\"OPENAI_API_KEY\") or \"\"\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"EnvironmentError: Failed to load `OPENAI_API_KEY`\")\n",
    "\n",
    "openai.api_key = api_key\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Chat Completion\n",
    "- [Create our own Code Interpreter | A hacker's guide to Language Models ](https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb)\n",
    "- [GPT - OpenAI API ](https://platform.openai.com/docs/guides/gpt/function-calling)\n",
    "- [Function-calling with an OpenAPI specification | OpenAI Cookbook](https://cookbook.openai.com/examples/function_calling_with_an_openapi_spec)\n",
    "\n",
    "Jeremy Howard provided an overview of using ChatGPT to create you own \"Code Interpreter\" that can be used by LLMs to call python code.\n",
    "\n",
    "See:\n",
    "https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import create_model\n",
    "import inspect, json\n",
    "from inspect import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.utils import nested_idx\n",
    "def response(compl): print(nested_idx(compl, 'choices', 0, 'message', 'content'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sums(a:int, b:int=1):\n",
    "    \"Adds a + b\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema(f):\n",
    "    kw = {n:(o.annotation, ... if o.default==Parameter.empty else o.default)\n",
    "          for n,o in inspect.signature(f).parameters.items()}\n",
    "    s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n",
    "    return dict(name=f.__name__, description=f.__doc__, parameters=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c5/gc7vgjds6tq1jy3b143hjtnm0000gn/T/ipykernel_3444/3240321801.py:4: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
      "  s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'sums',\n",
       " 'description': 'Adds a + b',\n",
       " 'parameters': {'properties': {'a': {'title': 'A', 'type': 'integer'},\n",
       "   'b': {'default': 1, 'title': 'B', 'type': 'integer'}},\n",
       "  'required': ['a'],\n",
       "  'title': 'Input for `sums`',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema(sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to disclose our OPEN_API_KEY to the world, so a good best practice is to put the key in a .env file that is ignored in the .gitignore so we don't accidentaly import it into GitHub. We use the python-dotenv package to import the OPENAI_API_KEY variable from the environment and assign it to the openai.api_key. For codespaces and docker containers, there are methods to handle secrets that get set to environment variables at startup so as to not accidentally leak information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askgpt(user, system=None, model=\"gpt-3.5-turbo\", **kwargs):\n",
    "    msgs = []\n",
    "    if system: msgs.append({\"role\": \"system\", \"content\": system})\n",
    "    msgs.append({\"role\": \"user\", \"content\": user})\n",
    "    return ChatCompletion.create(model=model, messages=msgs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c5/gc7vgjds6tq1jy3b143hjtnm0000gn/T/ipykernel_3444/3240321801.py:4: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
      "  s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n"
     ]
    }
   ],
   "source": [
    "c = askgpt(\"Use the `sum` function to solve this: What is 6+3?\",\n",
    "           system = \"You must use the `sum` function instead of adding yourself.\",\n",
    "           functions=[schema(sums)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject> JSON: {\n",
       "  \"role\": \"assistant\",\n",
       "  \"content\": null,\n",
       "  \"function_call\": {\n",
       "    \"name\": \"sums\",\n",
       "    \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = c.choices[0].message\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"a\": 6,\n",
      "  \"b\": 3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "k = m.function_call.arguments\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs_ok = {'sums', 'python'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_func(c):\n",
    "    fc = c.choices[0].message.function_call\n",
    "    if fc.name not in funcs_ok: return print(f'Not allowed: {fc.name}')\n",
    "    f = globals()[fc.name]\n",
    "    return f(**json.loads(fc.arguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_func(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(code):\n",
    "    tree = ast.parse(code)\n",
    "    last_node = tree.body[-1] if tree.body else None\n",
    "    \n",
    "    # If the last node is an expression, modify the AST to capture the result\n",
    "    if isinstance(last_node, ast.Expr):\n",
    "        tgts = [ast.Name(id='_result', ctx=ast.Store())]\n",
    "        assign = ast.Assign(targets=tgts, value=last_node.value)\n",
    "        tree.body[-1] = ast.fix_missing_locations(assign)\n",
    "\n",
    "    ns = {}\n",
    "    exec(compile(tree, filename='<ast>', mode='exec'), ns)\n",
    "    return ns.get('_result', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"\"\"\n",
    "a=1\n",
    "b=2\n",
    "a+b\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python(code:str):\n",
    "    \"Return result of executing `code` using python. If execution not permitted, returns `#FAIL#`\"\n",
    "    go = input(f'Proceed with execution?\\n```\\n{code}\\n```\\n')\n",
    "    if go.lower()!='y': return '#FAIL#'\n",
    "    return run(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c5/gc7vgjds6tq1jy3b143hjtnm0000gn/T/ipykernel_99382/3240321801.py:4: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
      "  s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n"
     ]
    }
   ],
   "source": [
    "c = askgpt(\"What is 12 factorial?\",\n",
    "           system = \"Use python for any required computations.\",\n",
    "           functions=[schema(python)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_func(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c5/gc7vgjds6tq1jy3b143hjtnm0000gn/T/ipykernel_42954/3240321801.py:4: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
      "  s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n"
     ]
    }
   ],
   "source": [
    "c = ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    functions=[schema(python)],\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is 12 factorial?\"},\n",
    "              {\"role\": \"function\", \"name\": \"python\", \"content\": \"479001600\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 factorial, denoted as 12!, is equal to 479,001,600.\n"
     ]
    }
   ],
   "source": [
    "response(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c5/gc7vgjds6tq1jy3b143hjtnm0000gn/T/ipykernel_42954/3240321801.py:4: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
      "  s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n"
     ]
    }
   ],
   "source": [
    "c = askgpt(\"What is the capital of France?\",\n",
    "           system = \"Use python for any required computations.\",\n",
    "           functions=[schema(python)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLama based agents and Llama.cpp functions\n",
    "[Zeng, Aohan, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2023. “AgentTuning: Enabling Generalized Agent Abilities for LLMs.” arXiv. http://arxiv.org/abs/2310.12823.\n",
    "](http://arxiv.org/abs/2310.12823)\n",
    "\n",
    "- [The Bloke Quantized](https://huggingface.co/TheBloke/agentlm-7B-GGUF/blob/main/agentlm-7b.Q4_K_M.gguf)\n",
    "\n",
    "- [lamma.cpp Grammers](https://github.com/ggerganov/llama.cpp/tree/master/grammars)\n",
    "- [Introducing Code Llama, a state-of-the-art large language model for coding](https://ai.meta.com/blog/code-llama-large-language-model-coding/)\n",
    "- [lamma.cpp Python](https://github.com/abetlen/llama-cpp-python)\n",
    "- [Implementation of Functions using Lamma.cpp grammar](https://github.com/teleprint-me/py.gpt.prompt/blob/main/docs/notebooks/llama_cpp_grammar_api.ipynb)\n",
    "- [Using grammars to constrain the llama.cpp output](https://www.imaurer.com/llama-cpp-grammars/)\n",
    "- [Simon Willioson Grammar](https://til.simonwillison.net/llms/llama-cpp-python-grammars)\n",
    "- [EasyLM Agent Tool Use](https://philschmid.github.io/easyllm/examples/llama2-agent-example/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from llama_cpp import ChatCompletionMessage, Llama, LlamaGrammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AgentLM\n",
    "Quantized Model from [\"AgentTuning: Enabling Generalized Agent Abilities For LLMs\"](https://thudm.github.io/AgentTuning/) published at [HuggingFace](https://huggingface.co/datasets/THUDM/AgentInstruct) from [The Bloke AgentLM](https://huggingface.co/TheBloke/agentlm-70B-GGUF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = Llama(model_path=\"./models/agentlm-70b.Q5_K_S.gguf\",n_gpu_layers=1,n_gqa=8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root ::= object \n",
      "object ::= [{] ws object_11 [}] ws \n",
      "value ::= object | array | string | number | value_6 ws \n",
      "array ::= [[] ws array_15 []] ws \n",
      "string ::= [\"] string_18 [\"] ws \n",
      "number ::= number_19 number_25 number_29 ws \n",
      "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_31 \n",
      "object_8 ::= string [:] ws value object_10 \n",
      "object_9 ::= [,] ws string [:] ws value \n",
      "object_10 ::= object_9 object_10 | \n",
      "object_11 ::= object_8 | \n",
      "array_12 ::= value array_14 \n",
      "array_13 ::= [,] ws value \n",
      "array_14 ::= array_13 array_14 | \n",
      "array_15 ::= array_12 | \n",
      "string_16 ::= [^\"\\] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "number_19 ::= number_20 number_21 \n",
      "number_20 ::= [-] | \n",
      "number_21 ::= [0-9] | [1-9] number_22 \n",
      "number_22 ::= [0-9] number_22 | \n",
      "number_23 ::= [.] number_24 \n",
      "number_24 ::= [0-9] number_24 | [0-9] \n",
      "number_25 ::= number_23 | \n",
      "number_26 ::= [eE] number_27 number_28 \n",
      "number_27 ::= [-+] | \n",
      "number_28 ::= [0-9] number_28 | [0-9] \n",
      "number_29 ::= number_26 | \n",
      "ws_30 ::= [ <U+0009><U+000A>] ws \n",
      "ws_31 ::= ws_30 | \n",
      "root ::= arr \n",
      "arr ::= [[] [<U+000A>] ws arr_12 []] \n",
      "value ::= object | array | string | number | value_7 ws \n",
      "object ::= [{] ws object_16 [}] ws \n",
      "array ::= [[] ws array_20 []] ws \n",
      "string ::= [\"] string_23 [\"] ws \n",
      "number ::= number_24 number_30 number_34 ws \n",
      "value_7 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_36 \n",
      "arr_9 ::= value arr_11 \n",
      "arr_10 ::= [,] [<U+000A>] ws value \n",
      "arr_11 ::= arr_10 arr_11 | \n",
      "arr_12 ::= arr_9 | \n",
      "object_13 ::= string [:] ws value object_15 \n",
      "object_14 ::= [,] ws string [:] ws value \n",
      "object_15 ::= object_14 object_15 | \n",
      "object_16 ::= object_13 | \n",
      "array_17 ::= value array_19 \n",
      "array_18 ::= [,] ws value \n",
      "array_19 ::= array_18 array_19 | \n",
      "array_20 ::= array_17 | \n",
      "string_21 ::= [^\"\\] | [\\] string_22 \n",
      "string_22 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_23 ::= string_21 string_23 | \n",
      "number_24 ::= number_25 number_26 \n",
      "number_25 ::= [-] | \n",
      "number_26 ::= [0-9] | [1-9] number_27 \n",
      "number_27 ::= [0-9] number_27 | \n",
      "number_28 ::= [.] number_29 \n",
      "number_29 ::= [0-9] number_29 | [0-9] \n",
      "number_30 ::= number_28 | \n",
      "number_31 ::= [eE] number_32 number_33 \n",
      "number_32 ::= [-+] | \n",
      "number_33 ::= [0-9] number_33 | [0-9] \n",
      "number_34 ::= number_31 | \n",
      "ws_35 ::= [ <U+0009><U+000A>] ws \n",
      "ws_36 ::= ws_35 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama_grammar = LlamaGrammar.from_file(\"json.gbnf\")\n",
    "import httpx\n",
    "grammar_text = httpx.get(\"https://raw.githubusercontent.com/ggerganov/llama.cpp/master/grammars/json_arr.gbnf\").text\n",
    "grammar = LlamaGrammar.from_string(grammar_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-2fb1ab75-9d03-4510-8e11-3efcae98ae8c', 'object': 'text_completion', 'created': 1699289335, 'model': './models/agentlm-70b.Q5_K_S.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? A: \\r, Jupiter, Saturn, Uranus, Neptune. ', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 15, 'completion_tokens': 17, 'total_tokens': 32}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13102.19 ms\n",
      "llama_print_timings:      sample time =       1.65 ms /    18 runs   (    0.09 ms per token, 10889.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13101.96 ms /    15 tokens (  873.46 ms per token,     1.14 tokens per second)\n",
      "llama_print_timings:        eval time =    2387.18 ms /    17 runs   (  140.42 ms per token,     7.12 tokens per second)\n",
      "llama_print_timings:       total time =   15516.23 ms\n"
     ]
    }
   ],
   "source": [
    "# Simple test of local LLM\n",
    "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=100, stop=[\"Q:\", \"\\n\"], echo=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-823cfa68-bf71-493b-92c7-63f60cef375b', 'object': 'text_completion', 'created': 1699287090, 'model': './models/agentlm-70b.Q5_K_S.gguf', 'choices': [{'text': 'Q: Construct an example of JSON-LD using schema.org vocabulary? A: ', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 23, 'completion_tokens': 1, 'total_tokens': 24}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  8651.60 ms\n",
      "llama_print_timings:      sample time =     0.80 ms /     1 runs   (    0.80 ms per token,  1256.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2898.22 ms /    22 tokens (  131.74 ms per token,     7.59 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  2904.15 ms\n"
     ]
    }
   ],
   "source": [
    "outputLD = llm(\"Q: Construct an example of JSON-LD using schema.org vocabulary? A: \", max_tokens=100, stop=[\"Q:\", \"\\n\"], echo=True)\n",
    "print(outputLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   40858.16 ms\n",
      "llama_print_timings:      sample time =     552.46 ms /    71 runs   (    7.78 ms per token,   128.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   40856.90 ms /    12 tokens ( 3404.74 ms per token,     0.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9720.15 ms /    70 runs   (  138.86 ms per token,     7.20 tokens per second)\n",
      "llama_print_timings:       total time =   51272.55 ms\n"
     ]
    }
   ],
   "source": [
    "response = llm(\n",
    "    \"JSON list of name strings of attractions in SF:\",\n",
    "    grammar=grammar, max_tokens=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"Fisherman's Wharf\",\n",
      "    \"Alcatraz Island\",\n",
      "    \"Golden Gate Park\",\n",
      "    \"Union Square\",\n",
      "    \"Chinatown\",\n",
      "    \"Haight-Ashbury\",\n",
      "    \"Lombard Street\",\n",
      "    \"Coit Tower\",\n",
      "    \"Pier 39\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(json.loads(response['choices'][0]['text']), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"@context\": \"<http://schema.org/>\",\n",
      "    \"@type\": \"PlaceList\",\n",
      "    \"itemListElement\": [\n",
      "        {\n",
      "            \"@type\": \"Attraction\",\n",
      "            \"name\": \"Golden Gate Bridge\"\n",
      "        },\n",
      "        {\n",
      "            \"@type\": \"Attraction\",\n",
      "            \"name\": \"Alcatraz Island\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   40858.16 ms\n",
      "llama_print_timings:      sample time =     497.41 ms /    64 runs   (    7.77 ms per token,   128.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5184.75 ms /    21 tokens (  246.89 ms per token,     4.05 tokens per second)\n",
      "llama_print_timings:        eval time =    8729.58 ms /    63 runs   (  138.56 ms per token,     7.22 tokens per second)\n",
      "llama_print_timings:       total time =   14585.59 ms\n"
     ]
    }
   ],
   "source": [
    "responseLD = llm(\n",
    "    \"JSON-LD list of name strings of attractions in SF as a key-value using schema.org:\",\n",
    "    grammar=llama_grammar, max_tokens=-1\n",
    ")\n",
    "print(json.dumps(json.loads(responseLD['choices'][0]['text']), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## React Pattern for LLMs\n",
    "\n",
    "Simion Wilison has a very simple blog post of implimenting the [ReAct: Synergizing Reasoning and Acting in Language Models](https://react-lm.github.io/) in one of his TILs -- [A simple Python implementation of the ReAct pattern for LLMs](https://til.simonwillison.net/llms/python-react-pattern). There are some issues in using regular expressions to select the tool the LLM want's to use. If the response can be constrained using a grammar as with the OpenAI model, more expressive response options are available. \n",
    "\n",
    "Simon has a nice description of this pattern from the Wordpress conference in Aug 2023. https://simonwillison.net/2023/Aug/27/wordcamp-llms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code is utilizing the `re` module in Python, which is used for working with regular expressions. The `re.compile` function compiles a regular expression pattern into a regular expression object, which can then be used to match against strings.\n",
    "\n",
    "Breaking down the regular expression `'^Action: (\\w+): (.*)$'`:\n",
    "\n",
    "- `^` asserts the start of a line. This means that the pattern will only match if it appears at the beginning of a string.\n",
    "  \n",
    "- `Action:` is a literal string. The pattern is looking for the word \"Action:\" exactly as it appears here.\n",
    "  \n",
    "- `\\w+` is a special sequence that matches any word character (equivalent to `[a-zA-Z0-9_]`) and the `+` means \"one or more\" of the preceding element. So, `\\w+` will match one or more word characters.\n",
    "  \n",
    "- `:` again, is a literal string, matching the colon character.\n",
    "  \n",
    "- `(.*)` is a capturing group that will match any character (except for line terminators) `.` zero or more times `*`. The `.` matches any single character except newline characters, and the `*` means \"zero or more\" of the preceding element. So this part of the pattern will capture everything after the second colon until the end of the line.\n",
    "  \n",
    "- `$` asserts the end of a line. This ensures that the pattern will match only if it extends all the way to the end of the string.\n",
    "\n",
    "When this regular expression is used to search a string, it looks for lines that start with \"Action:\", followed by a word character sequence that represents some action name, followed by a colon and then any sequence of characters after that until the end of the line.\n",
    "\n",
    "For example, the string \"Action: Save: Document 1\" would match this pattern. The `\\w+` would match \"Save\" and the `(.*)` would match \" Document 1\". These matched parts are in parentheses, which means they are captured groups that can be later extracted or referenced from the matched object.\n",
    "\n",
    "Here's what each group would capture in the given example:\n",
    "- Group 1 would capture \"Save\" (the action type)\n",
    "- Group 2 would capture \" Document 1\" (the action details)\n",
    "\n",
    "By compiling the regular expression with `re.compile`, the resulting regular expression object can be used for matching multiple times more efficiently, as the expression does not need to be recompiled each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is part of a Python script that uses the OpenAI API to create a chatbot that interacts with a user, potentially performs web-based actions like searching Wikipedia, performing calculations, or searching a blog, and then uses the results of those actions to respond to the user.\n",
    "\n",
    "The `action_re` regular expression is designed to parse lines from the chatbot's output that signify an \"Action\" that the bot decides to take. When the bot outputs a line starting with \"Action:\" followed by a word and a colon, then any text after that, the regular expression is used to capture this and process it accordingly.\n",
    "\n",
    "Here's the regular expression again:\n",
    "\n",
    "```python\n",
    "action_re = re.compile('^Action: (\\w+): (.*)$')\n",
    "```\n",
    "\n",
    "When this regular expression is applied to a string:\n",
    "\n",
    "1. `^Action:` matches the beginning of a line that starts with \"Action:\".\n",
    "2. `(\\w+)` captures one or more word characters immediately following \"Action:\", which represents the action type (like \"wikipedia\" or \"calculate\").\n",
    "3. `: ` matches the literal colon and space that follows the action type.\n",
    "4. `(.*)` captures everything following the colon and space, which represents the action input or query (like \"France\" for a Wikipedia search or \"4 * 7 / 3\" for a calculation).\n",
    "5. `$` asserts the end of the line, ensuring no extra characters are present after the pattern.\n",
    "\n",
    "The `query` function runs the chatbot's interaction. The chatbot can respond multiple times, and part of its response can include an \"Action\" line, which is meant to be executed by the script.\n",
    "\n",
    "When such a line is detected:\n",
    "\n",
    "1. The script extracts the action and the action input using the `.groups()` method on the match object.\n",
    "2. It checks if the action is in the `known_actions` dictionary. If it isn't, an exception is raised.\n",
    "3. If the action is known, the corresponding function (either `wikipedia`, `calculate`, or `simon_blog_search`) is called with the action input.\n",
    "4. The result of this action is then treated as an \"Observation\" and used in the next prompt to the chatbot.\n",
    "\n",
    "The functions `wikipedia`, `simon_blog_search`, and `calculate` are defined to perform the respective actions:\n",
    "\n",
    "- `wikipedia` uses `httpx` to make a GET request to the Wikipedia API and returns a search snippet.\n",
    "- `simon_blog_search` performs a search on a blog using a provided JSON API.\n",
    "- `calculate` uses Python's `eval` function to execute a calculation provided as a string.\n",
    "\n",
    "Security Note: The use of `eval` is generally discouraged as it can run arbitrary code. It's important to sanitize any input before using it in `eval` to prevent potential security vulnerabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is Apache 2 licensed:\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "import openai\n",
    "import re\n",
    "import httpx\n",
    "\n",
    " \n",
    "class ChatBot:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.system = system\n",
    "        self.messages = []\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "    \n",
    "    def __call__(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "    \n",
    "    def execute(self):\n",
    "        completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.messages)\n",
    "        # Uncomment this to print out token usage each time, e.g.\n",
    "        # {\"completion_tokens\": 86, \"prompt_tokens\": 26, \"total_tokens\": 112}\n",
    "        # print(completion.usage)\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "calculate:\n",
    "e.g. calculate: 4 * 7 / 3\n",
    "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
    "\n",
    "wikipedia:\n",
    "e.g. wikipedia: Django\n",
    "Returns a summary from searching Wikipedia\n",
    "\n",
    "simon_blog_search:\n",
    "e.g. simon_blog_search: Django\n",
    "Search Simon's blog for that term\n",
    "\n",
    "Always look things up on Wikipedia if you have the opportunity to do so.\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: What is the capital of France?\n",
    "Thought: I should look up France on Wikipedia\n",
    "Action: wikipedia: France\n",
    "PAUSE\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: France is a country. The capital is Paris.\n",
    "\n",
    "You then output:\n",
    "\n",
    "Answer: The capital of France is Paris\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "action_re = re.compile('^Action: (\\w+): (.*)$')\n",
    "\n",
    "def query(question, max_turns=5):\n",
    "    i = 0\n",
    "    bot = ChatBot(prompt)\n",
    "    next_prompt = question\n",
    "    while i < max_turns:\n",
    "        i += 1\n",
    "        result = bot(next_prompt)\n",
    "        print(result)\n",
    "        actions = [action_re.match(a) for a in result.split('\\n') if action_re.match(a)]\n",
    "        if actions:\n",
    "            # There is an action to run\n",
    "            action, action_input = actions[0].groups()\n",
    "            if action not in known_actions:\n",
    "                raise Exception(\"Unknown action: {}: {}\".format(action, action_input))\n",
    "            print(\" -- running {} {}\".format(action, action_input))\n",
    "            observation = known_actions[action](action_input)\n",
    "            print(\"Observation:\", observation)\n",
    "            next_prompt = \"Observation: {}\".format(observation)\n",
    "        else:\n",
    "            return\n",
    "\n",
    "\n",
    "def wikipedia(q):\n",
    "    return httpx.get(\"https://en.wikipedia.org/w/api.php\", params={\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": q,\n",
    "        \"format\": \"json\"\n",
    "    }).json()[\"query\"][\"search\"][0][\"snippet\"]\n",
    "\n",
    "\n",
    "def simon_blog_search(q):\n",
    "    results = httpx.get(\"https://datasette.simonwillison.net/simonwillisonblog.json\", params={\n",
    "        \"sql\": \"\"\"\n",
    "        select\n",
    "          blog_entry.title || ': ' || substr(html_strip_tags(blog_entry.body), 0, 1000) as text,\n",
    "          blog_entry.created\n",
    "        from\n",
    "          blog_entry join blog_entry_fts on blog_entry.rowid = blog_entry_fts.rowid\n",
    "        where\n",
    "          blog_entry_fts match escape_fts(:q)\n",
    "        order by\n",
    "          blog_entry_fts.rank\n",
    "        limit\n",
    "          1\"\"\".strip(),\n",
    "        \"_shape\": \"array\",\n",
    "        \"q\": q,\n",
    "    }).json()\n",
    "    return results[0][\"text\"]\n",
    "\n",
    "def calculate(what):\n",
    "    return eval(what)\n",
    "\n",
    "known_actions = {\n",
    "    \"wikipedia\": wikipedia,\n",
    "    \"calculate\": calculate,\n",
    "    \"simon_blog_search\": simon_blog_search\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: I need to look up the information about England's borders.\n",
      "Action: wikipedia: England\n",
      "PAUSE\n",
      " -- running wikipedia England\n",
      "Observation: <span class=\"searchmatch\">England</span> is a country that is part of the United Kingdom. It shares land borders with Wales to its west and Scotland to its north, while Ireland is located\n",
      "Answer: England shares borders with Wales to its west and Scotland to its north.\n"
     ]
    }
   ],
   "source": [
    "query(\"What does England share borders with?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Architecture\n",
    "[\"A Survey on Large Language Model based Autonomous Agents\"](https://arxiv.org/pdf/2308.11432.pdf) and [How Assistants Work -- OpenAI API](https://platform.openai.com/docs/assistants/how-it-works)\n",
    "\n",
    "| Unified Framework Component | Assistants API Equivalent | Description                                                                                         |\n",
    "|-----------------------------|----------------------------|-----------------------------------------------------------------------------------------------------|\n",
    "| Profile Module              | Assistant                 | Defines the AI's attributes, capabilities, and personality, setting the stage for its interactions. |\n",
    "| Memory Module               | Thread                    | Manages the history of interactions, storing and truncating messages to maintain context.           |\n",
    "| Planning Module             | Run                       | Processes the current state and plans the next actions, deciding how to utilize messages and tools. |\n",
    "| Action Module               | Message & Run Step        | Executes actions, generates responses, and may call tools to produce outputs or perform tasks.      |\n",
    "\n",
    "State transitions in the Assistants API would involve changes in the \"Thread\" as new \"Messages\" are added, and as the \"Assistant\" performs new \"Runs\" and \"Run Steps,\" updating its current knowledge and planned actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
